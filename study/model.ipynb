{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3情報科学演習：時系列データ分析（後編）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは, data.ipynbで処理を施したデータ(processed_data.csv)を用いて, 学習モデルを作成していく  \n",
    "今回は, RandomForestとLSTMという2つの学習モデルを実装・評価を行う  \n",
    "一言で, 学習モデルと言っても, 学習モデルは多数存在しており, その裏で動くアルゴリズムも異なる  \n",
    "更に言えば, 同じモデル名でもレイヤーの構成・パラメータ等でも性能に違いが出てくる  \n",
    "\n",
    "分からない単語やモデル名, コード等は逐一調べて, ふんわりでも良いので理解していくことが重要  \n",
    "また, あくまでこれは参考用に記述してモノであり, 厳密に言うと不適切だったり, 工夫の余地がまだまだ存在していたりするので, こういうやり方なんだなぁ程度で読んでいくのがおすすめ  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# Random Forest用\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# LSTM用\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import scipy.stats\n",
    "\n",
    "# 警告フィルター\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 〇 Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forestは決定木ベースのアンサンブル（バギング）モデル  \n",
    "決定木ベースなので完全な連続値の表現はできないが，複数の決定木により可能になる"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_data = pd.read_csv('./data/processed_data.csv', index_col=['UNIX_Time'])\n",
    "trainval_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価指標算出用\n",
    "result_score=['R^2', 'RMSE']\n",
    "def get_score(Y_true, Y_pred):\n",
    "    r2 = r2_score(Y_true, Y_pred)\n",
    "    rmse = mean_squared_error(Y_true, Y_pred, squared=False)\n",
    "    return round(r2, 3), round(rmse, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#目的変数を指定\n",
    "target_name = 'Radiation'\n",
    "\n",
    "#説明変数と目的変数に分割\n",
    "X_trainval_data, Y_trainval_data = trainval_data.drop([target_name], axis=1), trainval_data.loc[:,target_name]\n",
    "\n",
    "#学習データと検証データに分割\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_trainval_data, Y_trainval_data, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### パラメータ探索"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的にどんな学習器でも手動で決定するパラメータ（ハイパーパラメータ）が存在する  \n",
    "訓練・検証データを使って交差検証の結果から適したハイパーパラメータを見つける  \n",
    "チューニングもやり方が色々あるが，一番単純なグリッドサーチを使う  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パラメータ候補\n",
    "params_list = {\n",
    "    'max_depth' : [5, 6, 7],\n",
    "    'max_features': ['auto', None],\n",
    "    'n_estimators': [110, 120],\n",
    "    'random_state' : [0] #乱数固定用(探索不要)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchのクラス定義\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), params_list, cv=3, verbose=1)\n",
    "\n",
    "# 探索開始\n",
    "grid_search.fit(X_trainval_data, Y_trainval_data)\n",
    "\n",
    "# ベストパラメータ出力\n",
    "print(\"最良パラメータ: {}\".format(grid_search.best_params_))\n",
    "print(\"最良交差検証スコア: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# ベストパラメータを取得\n",
    "best_max_depth = grid_search.best_params_.get('max_depth')\n",
    "best_max_features = grid_search.best_params_.get('max_features')\n",
    "best_n_estimators = grid_search.best_params_.get('n_estimators')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習・検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シード値の固定用\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(11)\n",
    "random.seed(101)\n",
    "tf.random.set_seed(1001)\n",
    "\n",
    "# モデル構築\n",
    "model = RandomForestRegressor(\n",
    "    max_depth=best_max_depth, \n",
    "    max_features=best_max_features, \n",
    "    n_estimators=best_n_estimators,\n",
    "    )\n",
    "\n",
    "#学習実行\n",
    "model.fit(X_train, Y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "\n",
    "#検証実行\n",
    "pred_validation = model.predict(X_validation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = pd.DataFrame([get_score(Y_train, pred_train)], columns=result_score, index=['Train'])\n",
    "validation_score = pd.DataFrame([get_score(Y_validation, pred_validation)], columns=result_score, index=['Val'])\n",
    "\n",
    "df_result = pd.concat([train_score, validation_score], axis=0)\n",
    "df_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習・検証予測線の描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データの予測線\n",
    "cmap = plt.get_cmap('tab10')\n",
    "plt.figure(figsize=[16.5,3.5])\n",
    "plt.plot(Y_train.index, Y_train, c=cmap(0), linewidth=1.5, label='True')\n",
    "plt.plot(Y_train.index, pred_train, c=cmap(1), linewidth=1.5, label='Predict')\n",
    "plt.xticks(['2016-09-01 09:35:11-10:00', '2016-09-22 12:05:21-10:00', '2016-10-11 23:45:18-10:00', '2016-10-30 05:30:18-10:00', '2016-11-17 08:10:04-10:00'])\n",
    "plt.title('Train Predict')\n",
    "plt.xlabel('UNIX_Time')\n",
    "plt.ylabel('Radiation')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証データの予測線\n",
    "plt.figure(figsize=[16.5,3.5])\n",
    "plt.plot(Y_validation.index, Y_validation, c=cmap(0), linewidth=1.5, label='True')\n",
    "plt.plot(Y_validation.index, pred_validation, c=cmap(2), linewidth=1.5, label='Predict')\n",
    "plt.xticks(['2016-11-17 08:15:02-10:00', '2016-11-21 20:50:02-10:00', '2016-11-26 09:40:20-10:00', '2016-12-02 02:55:04-10:00', '2016-12-09 05:55:52-10:00'])\n",
    "plt.title('Validation Predict')\n",
    "plt.xlabel('UNIX_Time')\n",
    "plt.ylabel('Radiation')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特徴量重要度(おまけ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forestを使用した特徴量重要度を見てみる  \n",
    "決定木ベースのモデル（scikit-learn）は基本的に重要度が見れるようになっている  \n",
    "藤浪「どうやって重要度を求めてるのかは知らないので割愛」  \n",
    "B4原田「調べておきました」  \n",
    "\n",
    "<img src='./image/image2.jpg' width=600>\n",
    "\n",
    "実際には検証データに対して重要度を見て，不要な特徴量を除去するなど対処を行う  \n",
    "テストデータで見るのは自由だが，その結果から特徴量エンジニアリング等をするのは不正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(index=X_train.columns, data=model.feature_importances_)\n",
    "feature_importance.sort_values(inplace=True)\n",
    "\n",
    "plt.figure(figsize=[13,5])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(feature_importance.index, feature_importance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 〇 LSTM：Long Term Short Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM（Long Short Term Memory）はニューラルネットワークの一種で時系列データの学習を得意とするモデル  \n",
    "RNNと呼ばれるニューラルネットワークの一種でもあるが，RNNよりも長い時系列に対応するように設計されている  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/processed_data.csv').values\n",
    "data = data[:,1:]\n",
    "data = np.asarray(data).astype(np.float64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 関数定義"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMや1D-CNNなどの時系列モデルではサブシーケンス(ウィンドウ)形式でデータを入力する必要がある  \n",
    "サブシーケンスは部分系列を意味しており，一定期間のデータを1つのデータとしたもの  \n",
    "\n",
    "何時点前までのデータを見るかを決め，スライディングウィンドウ方式でデータを作成していく  \n",
    "ちなみに見る時点の範囲をルックバック数といい，この数はデータのドメイン知識から決めることがベターだと思う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM用のデータ整形\n",
    "def generate_data(data, input_data, output_data, sequence_size):\n",
    "    X_data=[]\n",
    "    Y_data=[]\n",
    "\n",
    "    #入力シーケンスサイズ\n",
    "    look_back = sequence_size\n",
    "\n",
    "    #入力シーケンス分だけ繰り返す\n",
    "    for i in range(data.shape[0]-look_back):\n",
    "        X_timedata=[]\n",
    "        Y_timedata=[]\n",
    "\n",
    "        if i+look_back+1 < data.shape[0]:\n",
    "            #サブキャリア分だけ繰り返す\n",
    "            for j in range(input_data.shape[1]):\n",
    "            #i~i+lookbackはシーケンス範囲, jは特徴量インデックス\n",
    "                X_timedata.append(input_data[i:i+look_back, j])\n",
    "            X_timedata = np.array(X_timedata)\n",
    "            X_timedata = X_timedata.transpose()\n",
    "            X_data.append(X_timedata)\n",
    "\n",
    "            #目的変数は1つのみなのでシーケンス分だけ繰り返す\n",
    "            Y_timedata.append(output_data[i+look_back-1])\n",
    "            Y_timedata = np.array(Y_timedata)\n",
    "            Y_timedata = Y_timedata.transpose()\n",
    "            Y_data.append(Y_timedata)\n",
    "\n",
    "    X_data = np.array(X_data)\n",
    "    Y_data = np.array(Y_data)\n",
    "\n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習・検証曲線の描画\n",
    "def make_graph(i, batch_size_list, history):\n",
    "    plt.figure(figsize=[6,3])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Batch_Size : ' + str(batch_size_list[i]))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テストデータ予測用(今は実行できない)\n",
    "def LSTM_test_predict(best_model, sequence_size):\n",
    "    test_data = pd.read_csv('./test/test_data.csv').values\n",
    "    test_data = test_data[:,1:]\n",
    "    test_data = np.asarray(test_data).astype(np.float64)\n",
    "\n",
    "    input_data = test_data[:,1:]\n",
    "    input_data = scipy.stats.zscore(input_data)\n",
    "    output_data = test_data[:,0]\n",
    "\n",
    "    X_test, Y_test = generate_data(test_data, input_data, output_data, sequence_size)\n",
    "    \n",
    "    pred_test = best_model.predict(X_test)\n",
    "    test_score = pd.DataFrame([get_score(Y_test, pred_test)], columns=result_score, index=['Test'])\n",
    "\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    plt.figure(figsize=[16.5,3.5])\n",
    "    plt.plot(range(0, len(Y_test)), Y_test, c=cmap(0), linewidth=1.5, label='True')\n",
    "    plt.plot(range(0, len(pred_test)), pred_test, c=cmap(3), linewidth=1.5, label='Predict')\n",
    "    plt.title('Test Predict')\n",
    "    plt.xlabel('UNIX_Time Index')\n",
    "    plt.ylabel('Radiation')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return pred_test, test_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習準備"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークを使用する際はデータを正規化，標準化することとが定石になっている  \n",
    "正規化することで学習速度を向上させることができる  \n",
    "理由としては，各データはスケールが異なる（単位が異なる）ことから元々の状態で予測値に対する影響度が違うため  \n",
    "値が大きいと誤差逆伝播により重み更新をする際に移動量が大きくなり，収束に時間がかかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#目的変数と説明変数に分割\n",
    "input_data = data[:,1:]\n",
    "input_data = scipy.stats.zscore(input_data) #説明変数のみを標準化\n",
    "output_data = data[:,0]\n",
    "\n",
    "#LSTM用にデータ加工\n",
    "sequence_size = 10\n",
    "X_trainval_data, Y_trainval_data = generate_data(data, input_data, output_data, sequence_size)\n",
    "\n",
    "#モデル構築用_変数設定\n",
    "X_dim = X_trainval_data.shape[2]\n",
    "Y_dim = Y_trainval_data.shape[1]\n",
    "\n",
    "#学習用_変数設定\n",
    "epochs_max = 100\n",
    "validation_split_rate = 0.2\n",
    "verbose_level = 0\n",
    "batch_size_min = 16\n",
    "batch_size_max = 64\n",
    "batch_size_list = []\n",
    "for a in range(int(math.log2(batch_size_min))-1, int(math.log2(batch_size_max))):\n",
    "    batch_size_list.append(2**(a+1))\n",
    "\n",
    "#データ分割(学習用と検証用)\n",
    "X_train = X_trainval_data[:-int(len(X_trainval_data)*(validation_split_rate))]\n",
    "Y_train = Y_trainval_data[:-int(len(Y_trainval_data)*(validation_split_rate))]\n",
    "X_validation = X_trainval_data[-int(len(X_trainval_data)*(validation_split_rate)):]\n",
    "Y_validation = Y_trainval_data[-int(len(Y_trainval_data)*(validation_split_rate)):]\n",
    "\n",
    "#変数の初期化\n",
    "best_r2 = -float('inf')\n",
    "best_rmse = float('inf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習・検証"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深層学習のフレームワークにはtensorflowとPytorchが有名  \n",
    "ライブラリで多少学習結果が変わるらしいがネットワーク構成は似たように書けるはずなので好きな方を使ってください  \n",
    "※今回は, tensorflowを使っています"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロスを監視して一定回数（patience）以上ロスが下がらない場合，学習を終了する  \n",
    "学習を早く終えるためではなく，過学習を防止するため  \n",
    "そのため検証データのロスを監視する  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習の実行\n",
    "df_result = pd.DataFrame()\n",
    "for i in range(len(batch_size_list)):\n",
    "    # シード値の固定用\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(11)\n",
    "    random.seed(101)\n",
    "    tf.random.set_seed(1001)\n",
    "\n",
    "    #モデル構築\n",
    "    print('\\n-- Model Construction ['+str(i+1)+'/'+str(len(batch_size_list))+'] --')\n",
    "    model = Sequential(name='LSTM_'+str(batch_size_list[i]))\n",
    "    model.add(LSTM(7, input_shape=(sequence_size, X_dim)))\n",
    "    model.add(Dense(Y_dim))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=0.01))\n",
    "    early_stopping = EarlyStopping(patience=10, monitor='val_loss', min_delta=0.01, restore_best_weights=True, verbose=0)\n",
    "    print('Model:LSTM_'+str(batch_size_list[i]))\n",
    "\n",
    "    #学習・検証\n",
    "    print('> Train and Validation')\n",
    "    history = model.fit(X_trainval_data, Y_trainval_data, batch_size=batch_size_list[i], epochs=epochs_max, validation_split=validation_split_rate, verbose=verbose_level, callbacks=[early_stopping])\n",
    "\n",
    "    #学習・検証曲線\n",
    "    make_graph(i, batch_size_list, history)\n",
    "\n",
    "    #学習結果\n",
    "    print('> Predict')\n",
    "    pred_validation = model.predict(X_validation, verbose=0)\n",
    "    pred_train = model.predict(X_train, verbose=0)\n",
    "\n",
    "    train_score = pd.DataFrame([get_score(Y_train, pred_train)], columns=result_score, index=['Train_'+str(batch_size_list[i])])\n",
    "    validation_score = pd.DataFrame([get_score(Y_validation, pred_validation)], columns=result_score, index=['Val_'+str(batch_size_list[i])])\n",
    "\n",
    "    df_result = pd.concat([df_result, train_score, validation_score])\n",
    "    r2_val, rmse_val = get_score(Y_validation, pred_validation)\n",
    "\n",
    "    #ベストモデル更新判定\n",
    "    print('> Best Model Judgement', end=\"\")\n",
    "    if(r2_val > best_r2):\n",
    "        best_r2 = r2_val\n",
    "        best_lstm_model = model\n",
    "        print(': Update\\n')\n",
    "    else:\n",
    "        print('\\n')\n",
    "\n",
    "print('\\n-- Train and Validation Curve --')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ 実行時間目安：46分  \n",
    "バッチサイズが小さい場合も，理想に近いLossの推移をすることが多い（イテレーション数が多い，局所解に陥りにくいから？）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習結果表示\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ベストモデル抽出\n",
    "for j in batch_size_list:\n",
    "    df_result.drop('Train_'+str(j), axis=0, inplace=True)\n",
    "df_result[df_result['R^2'].idxmax():df_result['R^2'].idxmax()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 訓練・検証予測線の描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#予測の実行\n",
    "pred_train = best_lstm_model.predict(X_train, verbose=0)\n",
    "pred_validation = best_lstm_model.predict(X_validation, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R^2に関するベストモデル(訓練データに対する予測グラフ)\n",
    "cmap = plt.get_cmap('tab10')\n",
    "plt.figure(figsize=[16.5,3.5])\n",
    "plt.plot(range(0, len(Y_train)), Y_train[:,0], c=cmap(0), linewidth=1.5, label=\"True\")\n",
    "plt.plot(range(0, len(pred_train)), pred_train[:,0], c=cmap(1), linewidth=1.5, label=\"predict\")\n",
    "plt.title('Train Predict by \"best_lstm_model\"')\n",
    "plt.xlabel('UNIX_Time Index')\n",
    "plt.ylabel('Radiation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R^2に関するベストモデル(検証データに対する予測グラフ)\n",
    "plt.figure(figsize=[16.5,3.5])\n",
    "plt.plot(range(0, len(Y_validation)), Y_validation[:,0], c=cmap(0), linewidth=1.5, label=\"True\")\n",
    "plt.plot(range(0, len(pred_validation)), pred_validation[:,0], c=cmap(2), linewidth=1.5, label=\"predict\")\n",
    "plt.title('Validation Predict by \"best_lstm_model\"')\n",
    "plt.xlabel('UNIX_Time Index')\n",
    "plt.ylabel('Radiation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### さいごに"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを提供されたところから実際に機械学習で予測するところまでの一連の流れをやってみた  \n",
    "当然だがこれはあくまで一例，思い付きで処理をしているだけなので各ステップで色々と考察すべきところがある  \n",
    "\n",
    "時系列データは関係ないが，基本的にはここでやったようにウォーターフォール型で各ステップをこなしていくため，後戻りが面倒  \n",
    "そのため、自分一人の考えで全てを進めていくのではなく，都度誰かに相談したり，進捗内容を関係者に共有したりしながら無駄のないように進めていくのが大事  \n",
    "（といっても基本的に自分一人）  \n",
    "\n",
    "何か研究テーマのヒントにでもなってれば幸いです"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 採点ポイント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価時には, その人が実際に研究に取り組んだ時のことを想像されることを意識してください  \n",
    "何をすれば評価されるというのは明確に決まっているわけではなく, 当然タイミング等の運要素もありますが, 基本的には相対的加点方式になると思います  \n",
    "\n",
    "以下, 大まかな採点ポイント  \n",
    "\n",
    "- 学習モデルの精度\n",
    "- 進捗の量と質\n",
    "- 進捗報告の質(量ではないことに注意)\n",
    "- アイデア力\n",
    "- 合理性と論理的な判断思考力\n",
    "\n",
    "最終的な学習モデルの精度は言うまでもなく採点対象です  \n",
    "\n",
    "進捗自体の量と質を正確に把握することは困難ですので, 毎週の進捗報告のスライドから判断せざるを得ません  \n",
    "いかに, 限られた時間の中で自分のやったことを簡潔かつ正確に伝えることができるのかも非常に大事になってきます  \n",
    "やったことを全てアピールしたい気持ちも分かりますが, 聞き手に配慮したスライド作りを期待しています  \n",
    "スライドなんてとか思ってると, 共同研究のミーティングや対外発表等での発表時間制限で痛い目を見ます  \n",
    "\n",
    "以下, ↓B4原田の当時の進捗スライドの一部↓\n",
    "\n",
    "<img src='./image/image3.jpg' width=400>\n",
    "\n",
    "<img src='./image/image4.jpg' width=400>\n",
    "\n",
    "また, アイデアは結果の良し悪し関係なく, アイデアに裏付けられる合理的かつ論理的な思考過程があれば, 着眼点の鋭さを評価します  \n",
    "\n",
    "「ここのサイトに書いてあったから, 真似したら精度が上がりました」「なんとなくうまくいくような感じがしたので」等の理由では聞き手も納得できないことは容易に想像できると思います  \n",
    "しかし, 仮説を立てて実際に実験してみるという正しい手順を踏むと時間が足りないのも現実問題としてあります  \n",
    "自分の出した進捗に対して, なぜうまくいったのかを調べたり, 考察を深めたりすることで, 進捗報告の際にはあたかも初めからそのような思考をしていて, 実際にその仮説を元にやってみると本当に精度が上がったという風に伝えることで聞き手も感心しますし, 合理的かつ論理的な判断思考力が評価されます  \n",
    "\n",
    "まとめると, 約2ヵ月という限られた時間の中でなるべく効率的に要領良く取り組み, そこで生まれた結果を週に1回の数分(約5分)の間で聞き手に正確に伝えるということを心がけてください  \n",
    "健闘を祈ります  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b71ca67855a6852a504caa4c82db18f9372d030f98e645580c1e48a94b8f51fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
